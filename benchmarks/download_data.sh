#!/usr/bin/env bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# download_data.sh â€” Download NYC Taxi trip parquet files for benchmarks
#
# Source: s3://altinity-clickhouse-data/nyc_taxi_rides/data/tripdata_parquet/
#         (public bucket, no credentials required)
#
# Usage:
#   ./benchmarks/download_data.sh            # downloads 5 files (~1.1 GB)
#   ./benchmarks/download_data.sh 3          # downloads first 3 files
#   ./benchmarks/download_data.sh all        # downloads ALL 96 files (~21 GB)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
set -euo pipefail

S3_BUCKET="s3://altinity-clickhouse-data/nyc_taxi_rides/data/tripdata_parquet"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DATA_DIR="${SCRIPT_DIR}/data"

# Files to download (5 months spanning different years for variety)
DEFAULT_FILES=(
    "data-200901.parquet"   # Jan 2009 â€” early dataset, ~222 MB
    "data-201206.parquet"   # Jun 2012 â€” mid-era dataset
    "data-201501.parquet"   # Jan 2015 â€” recent-era dataset
    "data-201706.parquet"   # Jun 2017 â€” late-era dataset
    "data-201901.parquet"   # Jan 2019 â€” latest-era dataset
)

# â”€â”€ Parse arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COUNT="${1:-5}"

# â”€â”€ Pre-flight checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ! command -v aws &>/dev/null; then
    echo "âŒ  AWS CLI not found. Install it:"
    echo "    brew install awscli          # macOS"
    echo "    pip install awscli           # pip"
    echo "    https://aws.amazon.com/cli/  # other"
    exit 1
fi

mkdir -p "${DATA_DIR}"

# â”€â”€ Download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
download_file() {
    local file="$1"
    local dest="${DATA_DIR}/${file}"

    if [[ -f "${dest}" ]]; then
        echo "â­  Already exists: ${file}"
        return 0
    fi

    echo "â¬‡  Downloading ${file} â€¦"
    aws s3 cp "${S3_BUCKET}/${file}" "${dest}" --no-sign-request --no-progress
    echo "âœ…  Downloaded: ${file} ($(du -h "${dest}" | cut -f1))"
}

if [[ "${COUNT}" == "all" ]]; then
    echo "ğŸ“¦  Downloading ALL parquet files from the NYC taxi dataset â€¦"
    echo ""
    # List all files and download each
    aws s3 ls "${S3_BUCKET}/" --no-sign-request \
        | awk '{print $4}' \
        | grep '\.parquet$' \
        | while read -r file; do
            download_file "${file}"
        done
else
    # Clamp to available default files
    if (( COUNT > ${#DEFAULT_FILES[@]} )); then
        COUNT=${#DEFAULT_FILES[@]}
    fi

    echo "ğŸ“¦  Downloading ${COUNT} NYC taxi parquet file(s) for benchmarks â€¦"
    echo ""

    for (( i=0; i<COUNT; i++ )); do
        download_file "${DEFAULT_FILES[$i]}"
    done
fi

echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
FILE_COUNT=$(find "${DATA_DIR}" -name '*.parquet' | wc -l | tr -d ' ')
TOTAL_SIZE=$(du -sh "${DATA_DIR}" 2>/dev/null | cut -f1)
echo "ğŸ“  Data directory : ${DATA_DIR}"
echo "ğŸ“„  Files ready    : ${FILE_COUNT}"
echo "ğŸ’¾  Total size     : ${TOTAL_SIZE}"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "Run the benchmark:"
echo "  uv run python benchmarks/run_benchmark.py"
